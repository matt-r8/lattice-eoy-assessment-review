# Global Reviewer Quality Analysis - 2025 EOY

## Executive Summary

- **Total reviewers analyzed**: 143
- **Total reviews written**: 860
- **Total scores given**: 8864
- **Company average score**: 4.13
- **Firewall 5s identified**: 11 (7.7%)
- **Low-effort reviewers**: 6 (4.2%)
- **High-quality reviewers** (top 25): 25

## Firewall 5s Reviewers

Reviewers who gave 90%+ of their scores as 5.0 across ALL reviewees.

| Reviewer | % of 5.0s | Reviews Written | Total Scores | Mean Score | Std Dev | Flag |
|----------|-----------|-----------------|--------------|------------|---------|------|
| Dylan Doub | 100.0% | 6 | 66 | 5.00 | 0.00 | ğŸš¨ |
| Stefan Tanenbaum | 100.0% | 2 | 22 | 5.00 | 0.00 | ğŸš¨ |
| Ainsilie Hibbard | 100.0% | 2 | 21 | 5.00 | 0.00 | ğŸš¨ |
| Samuel McQueen | 100.0% | 5 | 55 | 5.00 | 0.00 | ğŸš¨ |
| Sean Herbert | 100.0% | 2 | 20 | 5.00 | 0.00 | ğŸš¨ |
| Hannah Cheng | 98.4% | 6 | 63 | 4.98 | 0.13 | ğŸš¨ |
| Kevin Nash | 95.0% | 2 | 20 | 4.95 | 0.22 | ğŸš¨ |
| Noah McHugh | 93.8% | 9 | 96 | 4.93 | 0.30 | âš ï¸ |
| David Alvarado | 92.2% | 6 | 64 | 4.92 | 0.27 | âš ï¸ |
| Minh Nguyen | 90.9% | 7 | 66 | 4.91 | 0.29 | âš ï¸ |
| Basudev Rijal | 90.9% | 1 | 11 | 4.91 | 0.30 | âš ï¸ |

## Low-Effort Reviewers

Reviewers with minimal text feedback (< 50 words per review) AND low score variance (< 0.5).

| Reviewer | Words per Review | Score Std Dev | Reviews Written | Mean Score | Flags |
|----------|------------------|---------------|-----------------|------------|-------|
| Pedro Torres | 23.8 | 0.30 | 8 | 4.90 | ğŸ“ Very Low Text |
| Dylan Doub | 26.9 | 0.00 | 6 | 5.00 | ğŸ“ Very Low Text, ğŸ“Š No Variance |
| David Alvarado | 31.0 | 0.27 | 6 | 4.92 | ğŸ“Š No Variance |
| Noah McHugh | 34.3 | 0.30 | 9 | 4.93 | ğŸ“Š No Variance |
| Samuel McQueen | 36.0 | 0.00 | 5 | 5.00 | ğŸ“Š No Variance |
| Brandon Shouse | 41.5 | 0.42 | 6 | 4.83 | âš ï¸ |

## High-Quality Reviewers (Top 25)

Reviewers demonstrating thoughtful engagement and differentiation.

| Rank | Reviewer | Quality Score | Words per Review | Score Std Dev | Reviews Written | Mean Score |
|------|----------|---------------|------------------|---------------|-----------------|------------|
| 1 | Jordan Dilworth | 100.0 | 186.0 | 1.08 | 10 | 3.31 |
| 2 | Joshua Pritchett | 98.4 | 169.8 | 0.95 | 10 | 4.06 |
| 3 | Danny Benson | 98.2 | 158.5 | 0.94 | 9 | 3.83 |
| 4 | Asare Nkansah | 96.7 | 176.2 | 0.89 | 9 | 4.04 |
| 5 | Luke Strebel | 94.2 | 273.2 | 0.81 | 11 | 4.35 |
| 6 | Bryon Kroger | 92.8 | 127.2 | 0.91 | 13 | 4.12 |
| 7 | Alexandra Brierton | 92.5 | 170.6 | 0.83 | 7 | 3.66 |
| 8 | Jeff Wills | 90.5 | 130.9 | 0.89 | 7 | 4.42 |
| 9 | Jeremy Steinbeck | 89.3 | 97.5 | 1.00 | 11 | 4.03 |
| 10 | Matt Pacione | 89.1 | 170.4 | 0.64 | 8 | 4.18 |
| 11 | Carlo Viray | 89.0 | 242.9 | 0.63 | 9 | 4.64 |
| 12 | Chris Wang | 88.2 | 163.5 | 0.86 | 5 | 3.84 |
| 13 | Jennifer Lopez | 87.8 | 151.4 | 0.93 | 4 | 4.02 |
| 14 | Dave Chapman | 86.0 | 178.5 | 0.70 | 6 | 3.82 |
| 15 | Vin Foregard | 85.7 | 128.4 | 0.75 | 7 | 4.04 |
| 16 | Norman Sharpe | 85.4 | 159.0 | 0.76 | 5 | 3.96 |
| 17 | Adam Gardner | 85.2 | 178.2 | 0.59 | 7 | 3.34 |
| 18 | Brian Jennings | 84.5 | 136.0 | 0.58 | 8 | 4.53 |
| 19 | Rob Monroe | 84.1 | 89.6 | 0.87 | 9 | 3.70 |
| 20 | Kristin Pearson | 83.6 | 85.1 | 0.89 | 10 | 4.39 |
| 21 | Dan Bitter | 83.2 | 114.4 | 0.68 | 8 | 3.49 |
| 22 | Michael Silverman | 83.1 | 126.8 | 0.76 | 6 | 3.95 |
| 23 | Ron Golan | 82.7 | 75.9 | 1.02 | 7 | 3.84 |
| 24 | Tom Anastasio | 82.7 | 150.0 | 0.67 | 5 | 4.42 |
| 25 | Steven Bair | 82.4 | 123.3 | 0.67 | 7 | 4.40 |

## Score Distribution Analysis

**Company Average Score**: 4.13

### Overall Score Distribution

| Score | Count | Percentage |
|-------|-------|------------|
| 1.0 | 6 | 0.1% |
| 2.0 | 221 | 2.5% |
| 3.0 | 1882 | 21.2% |
| 4.0 | 3225 | 36.4% |
| 5.0 | 3530 | 39.8% |

### Reviewer Score Pattern Categories

- **Harsh reviewers** (mean < 3.63): 23 (16.1%)
- **Generous reviewers** (mean > 4.63): 34 (23.8%)
- **Calibrated reviewers** (within Â±0.5 of average): 86 (60.1%)

## Insights & Recommendations

### Key Findings

1. **Firewall 5s Impact**: 11 reviewers (7.7%) are giving 90%+ of scores as 5.0, which reduces the signal-to-noise ratio in performance differentiation.

2. **Low-Effort Reviews**: 6 reviewers (4.2%) are providing minimal text feedback (< 50 words per review) with little score variance (< 0.5 std dev), suggesting checkbox completion rather than thoughtful assessment.

3. **Score Distribution**: The company average of 4.13 suggests a generally positive review culture, with 3530 scores of 5.0 (39.8% of all scores).

4. **High-Quality Reviewers**: The top 25 reviewers (17% of total) demonstrate significantly higher engagement with 150+ words per review on average and good score differentiation.

### Recommendations for Calibration

1. **Address Firewall 5s**: Consider providing calibration training to reviewers who consistently give 90%+ scores as 5.0. They may:
   - Not understand the rating scale
   - Be conflict-averse
   - Lack context to differentiate performance

2. **Improve Low-Effort Reviews**: Encourage reviewers with minimal feedback to:
   - Provide specific examples
   - Focus on actionable feedback
   - Use the full rating scale appropriately

3. **Recognize High-Quality Reviewers**: The top 25 reviewers provide valuable, thoughtful feedback. Consider:
   - Sharing examples of high-quality reviews (anonymized)
   - Using them as calibration references
   - Acknowledging their effort

4. **Rating Scale Calibration**: With a company average of 4.13, consider whether:
   - The scale is being used as intended
   - There's grade inflation
   - Calibration sessions would help reviewers differentiate performance levels

### Data Quality Notes

- Scores of 6.0 ('Haven't had opportunity to observe') were excluded from analysis
- Text feedback analyzed includes Question 11 (rehire comment) and Question 12 (START/STOP/KEEP) combined
- 'Words per review' represents the average combined word count from Q11 + Q12 text feedback
- Reviewers with < 3 reviews were not flagged as low-effort (insufficient sample size)
- Quality score is a composite metric (0-100) based on words per review, score differentiation, review volume, and calibration
