# Damon Redding - Individual Assessment Report

## Employee Information
- **Name**: Damon Redding
- **Department**: Design
- **Level**: Practitioner III
- **Team**: Starfox
- **Project**: KM - Section 31 & Platform (Tecolote)
- **Assessment Period**: 2025 EOY
- **Tier Assignment**: A- (A Player Baseline)

---

## Overall Scores

- **Peers Average**: 4.03 (based on 36 ratings)
- **Response Rate**: 4/4 peer reviewers (100%)
- **Self Average**: 3.73
- **Delta (Self - Peers)**: -0.30 ðŸŸ¢ (Humble)
- **Delta (Self - Level)**: -0.27 ðŸŸ¢ (Humble) (vs Level Average 4.00)
- **Delta (Self - Team)**: -0.53 ðŸŸ¢ (Humble) (vs Team Average 4.26)
- **Delta (Self - Department)**: -0.39 ðŸŸ¢ (Humble) (vs Department Average 4.12)
- **Delta (Self - Project)**: -0.39 ðŸŸ¢ (Humble) (vs Project Average 4.12)

---

## Accomplishments Review (Self-Assessment Question 13)

### Synthesized Summary

*[AI synthesis needed - invoke rise8-assessment-reviewer agent with accomplishments text]*

<details>
<summary>View Raw Accomplishments Text</summary>

1. 
   1. **Key Action: Ran and organized full team research visits** (Quake, Malibu), integrating PMs, developers, and designers directly with users.
   2. **Mission Impact:** Ensures we build **valuable software** by rapidly incorporating direct user needs and accelerating the feedback loop.
2. 
   1. **Key Action: Blitzar application received external recognition** at the Wideband and NarrowBand Working Group for its **intuitiveness and UX**.
   2. **Mission Impact:** Provides external proof that we deliver software that **users love**, enhancing our reputation and driving measurable user outcomes.
3. 
   1. **Key Action: Successfully onboarded and delivered results** under high pressure (Quake) and executed a **seamless switch** to the Malibu team.
   2. **Mission Impact:** Demonstrates the **high adaptability**, ensuring momentum is never disrupted by team transitions.

</details>

---

## Per-Question Breakdown

### Question 1: Be Bold

**Basic Scores:**
- **Peers Average**: 3.67 (based on 3 ratings)
- **Response Rate**: 3/4 peer reviewers (75%)
- **Self Score**: 3.0
- **Median Score**: 4.00

**Self-Awareness Deltas:**
- **Delta (Self - Peers)**: -0.67 ðŸŸ¢ (Humble)
- **Delta (Self - Level)**: -0.83 ðŸŸ¢ (Humble)
- **Delta (Self - Team)**: -0.83 ðŸŸ¢ (Humble)
- **Delta (Self - Department)**: -0.96 ðŸŸ¢ (Humble)
- **Delta (Self - Project)**: -0.77 ðŸŸ¢ (Humble)

**Performance Comparisons (Peer Avg vs Groups):**
- **vs Team Average**: -0.17 (Team avg: 3.83) - Below team
- **vs Project Average**: -0.11 (Project avg: 3.77) - Below project
- **vs Department Average**: -0.29 (Department avg: 3.96) - Below department
- **vs Company Average**: -0.36 (Company avg: 4.03) - Below company

**Statistical Analysis:**
- **Standard Deviation**: 0.47 (Moderate agreement)
- **Percentile Rank**: 22.5th percentile
- **Score Range**: 3.0 - 4.0 (spread: 1.0)

**Interpretation:**
- Scores in the **22th percentile** on Be Bold company-wide (top 78%)
- Peer average (3.67) is **-0.36 below company average**
- **Moderate agreement** among reviewers (SD: 0.47)
- Score range (3.0-4.0) shows reasonable consistency in reviewer perceptions

### Question 2: Do The Right Thing

**Basic Scores:**
- **Peers Average**: 4.00 (based on 4 ratings)
- **Response Rate**: 4/4 peer reviewers (100%)
- **Self Score**: 3.0
- **Median Score**: 4.00

**Self-Awareness Deltas:**
- **Delta (Self - Peers)**: -1.00 ðŸŸ¢ (Humble)
- **Delta (Self - Level)**: -1.15 ðŸŸ¢ (Humble)
- **Delta (Self - Team)**: -1.50 ðŸŸ¢ (Humble)
- **Delta (Self - Department)**: -1.22 ðŸŸ¢ (Humble)
- **Delta (Self - Project)**: -1.29 ðŸŸ¢ (Humble)

**Performance Comparisons (Peer Avg vs Groups):**
- **vs Team Average**: -0.50 (Team avg: 4.50) - Below team
- **vs Project Average**: -0.29 (Project avg: 4.29) - Below project
- **vs Department Average**: -0.22 (Department avg: 4.22) - Below department
- **vs Company Average**: -0.22 (Company avg: 4.22) - Below company

**Statistical Analysis:**
- **Standard Deviation**: 0.71 (Mixed opinions)
- **Percentile Rank**: 29.7th percentile
- **Score Range**: 3.0 - 5.0 (spread: 2.0)

**Interpretation:**
- Scores in the **29th percentile** on Do The Right Thing company-wide (top 70%)
- Peer average (4.00) is **-0.50 below team average**
- **Mixed opinions** among reviewers (SD: 0.71)
- Score range (3.0-5.0) shows notable variability in reviewer perceptions

### Question 3: Do What Works

**Basic Scores:**
- **Peers Average**: 4.00 (based on 3 ratings)
- **Response Rate**: 3/4 peer reviewers (75%)
- **Self Score**: 4.0
- **Median Score**: 4.00

**Self-Awareness Deltas:**
- **Delta (Self - Peers)**: +0.00 ðŸŸ¡ (Well-calibrated)
- **Delta (Self - Level)**: +0.21 ðŸ”´ (Overconfident)
- **Delta (Self - Team)**: +0.25 ðŸ”´ (Overconfident)
- **Delta (Self - Department)**: -0.08 ðŸŸ¡ (Well-calibrated)
- **Delta (Self - Project)**: +0.33 ðŸ”´ (Overconfident)

**Performance Comparisons (Peer Avg vs Groups):**
- **vs Team Average**: +0.25 (Team avg: 3.75) - Above team
- **vs Project Average**: +0.33 (Project avg: 3.67) - Above project
- **vs Department Average**: -0.08 (Department avg: 4.08) - Below department
- **vs Company Average**: +0.03 (Company avg: 3.97) - At company

**Statistical Analysis:**
- **Standard Deviation**: 0.00 (High consensus)
- **Percentile Rank**: 52.1th percentile
- **Score Range**: 4.0 - 4.0 (spread: 0.0)

*Perfect consensus - all reviewers gave identical scores*

### Question 4: Do What is Required

**Basic Scores:**
- **Peers Average**: 4.00 (based on 4 ratings)
- **Response Rate**: 4/4 peer reviewers (100%)
- **Self Score**: 4.0
- **Median Score**: 4.00

**Self-Awareness Deltas:**
- **Delta (Self - Peers)**: +0.00 ðŸŸ¡ (Well-calibrated)
- **Delta (Self - Level)**: -0.03 ðŸŸ¡ (Well-calibrated)
- **Delta (Self - Team)**: -0.25 ðŸŸ¢ (Humble)
- **Delta (Self - Department)**: -0.04 ðŸŸ¡ (Well-calibrated)
- **Delta (Self - Project)**: -0.15 ðŸŸ¢ (Humble)

**Performance Comparisons (Peer Avg vs Groups):**
- **vs Team Average**: -0.25 (Team avg: 4.25) - Below team
- **vs Project Average**: -0.15 (Project avg: 4.15) - Below project
- **vs Department Average**: -0.04 (Department avg: 4.04) - At department
- **vs Company Average**: -0.10 (Company avg: 4.10) - Below company

**Statistical Analysis:**
- **Standard Deviation**: 0.71 (Mixed opinions)
- **Percentile Rank**: 40.1th percentile
- **Score Range**: 3.0 - 5.0 (spread: 2.0)

**Interpretation:**
- Scores in the **40th percentile** on Do What is Required company-wide (top 60%)
- Peer average (4.00) is **-0.25 below team average**
- **Mixed opinions** among reviewers (SD: 0.71)
- Score range (3.0-5.0) shows notable variability in reviewer perceptions

### Question 5: Always Be Kind

**Basic Scores:**
- **Peers Average**: 3.75 (based on 4 ratings)
- **Response Rate**: 4/4 peer reviewers (100%)
- **Self Score**: 4.0
- **Median Score**: 4.00

**Self-Awareness Deltas:**
- **Delta (Self - Peers)**: +0.25 ðŸ”´ (Overconfident)
- **Delta (Self - Level)**: -0.11 ðŸŸ¢ (Humble)
- **Delta (Self - Team)**: +0.12 ðŸ”´ (Overconfident)
- **Delta (Self - Department)**: -0.18 ðŸŸ¢ (Humble)
- **Delta (Self - Project)**: +0.04 ðŸŸ¡ (Well-calibrated)

**Performance Comparisons (Peer Avg vs Groups):**
- **vs Team Average**: -0.12 (Team avg: 3.88) - Below team
- **vs Project Average**: -0.21 (Project avg: 3.96) - Below project
- **vs Department Average**: -0.43 (Department avg: 4.18) - Below department
- **vs Company Average**: -0.39 (Company avg: 4.14) - Below company

**Statistical Analysis:**
- **Standard Deviation**: 0.43 (Moderate agreement)
- **Percentile Rank**: 19.6th percentile
- **Score Range**: 3.0 - 4.0 (spread: 1.0)

**Interpretation:**
- Scores in the **19th percentile** on Always Be Kind company-wide (top 80%)
- Peer average (3.75) is **-0.43 below department average**
- **Moderate agreement** among reviewers (SD: 0.43)
- Score range (3.0-4.0) shows reasonable consistency in reviewer perceptions

### Question 6: Keep it Real

**Basic Scores:**
- **Peers Average**: 4.00 (based on 3 ratings)
- **Response Rate**: 3/4 peer reviewers (75%)
- **Self Score**: 4.0
- **Median Score**: 4.00

**Self-Awareness Deltas:**
- **Delta (Self - Peers)**: +0.00 ðŸŸ¡ (Well-calibrated)
- **Delta (Self - Level)**: -0.11 ðŸŸ¢ (Humble)
- **Delta (Self - Team)**: -0.50 ðŸŸ¢ (Humble)
- **Delta (Self - Department)**: -0.09 ðŸŸ¡ (Well-calibrated)
- **Delta (Self - Project)**: -0.21 ðŸŸ¢ (Humble)

**Performance Comparisons (Peer Avg vs Groups):**
- **vs Team Average**: -0.50 (Team avg: 4.50) - Below team
- **vs Project Average**: -0.21 (Project avg: 4.21) - Below project
- **vs Department Average**: -0.09 (Department avg: 4.09) - Below department
- **vs Company Average**: -0.17 (Company avg: 4.17) - Below company

**Statistical Analysis:**
- **Standard Deviation**: 0.00 (High consensus)
- **Percentile Rank**: 34.6th percentile
- **Score Range**: 4.0 - 4.0 (spread: 0.0)

*Perfect consensus - all reviewers gave identical scores*

### Question 7: Outcomes in Production

**Basic Scores:**
- **Peers Average**: 3.50 (based on 2 ratings)
- **Response Rate**: 2/4 peer reviewers (50%)
- **Self Score**: 3.0
- **Median Score**: 3.50

**Self-Awareness Deltas:**
- **Delta (Self - Peers)**: -0.50 ðŸŸ¢ (Humble)
- **Delta (Self - Level)**: -0.89 ðŸŸ¢ (Humble)
- **Delta (Self - Team)**: -0.75 ðŸŸ¢ (Humble)
- **Delta (Self - Department)**: -0.98 ðŸŸ¢ (Humble)
- **Delta (Self - Project)**: -0.60 ðŸŸ¢ (Humble)

**Performance Comparisons (Peer Avg vs Groups):**
- **vs Team Average**: -0.25 (Team avg: 3.75) - Below team
- **vs Project Average**: -0.10 (Project avg: 3.60) - Below project
- **vs Department Average**: -0.48 (Department avg: 3.98) - Below department
- **vs Company Average**: -0.48 (Company avg: 3.98) - Below company

**Statistical Analysis:**
- **Standard Deviation**: 0.50 (Moderate agreement)
- **Percentile Rank**: 18.1th percentile
- **Score Range**: 3.0 - 4.0 (spread: 1.0)

*Insufficient data for interpretation (minimum 3 responses needed)*

### Question 8: Grit

**Basic Scores:**
- **Peers Average**: 3.67 (based on 3 ratings)
- **Response Rate**: 3/4 peer reviewers (75%)
- **Self Score**: 4.0
- **Median Score**: 4.00

**Self-Awareness Deltas:**
- **Delta (Self - Peers)**: +0.33 ðŸ”´ (Overconfident)
- **Delta (Self - Level)**: -0.04 ðŸŸ¡ (Well-calibrated)
- **Delta (Self - Team)**: -0.33 ðŸŸ¢ (Humble)
- **Delta (Self - Department)**: -0.01 ðŸŸ¡ (Well-calibrated)
- **Delta (Self - Project)**: -0.20 ðŸŸ¢ (Humble)

**Performance Comparisons (Peer Avg vs Groups):**
- **vs Team Average**: -0.67 (Team avg: 4.33) - Below team
- **vs Project Average**: -0.54 (Project avg: 4.20) - Below project
- **vs Department Average**: -0.34 (Department avg: 4.01) - Below department
- **vs Company Average**: -0.49 (Company avg: 4.16) - Below company

**Statistical Analysis:**
- **Standard Deviation**: 0.47 (Moderate agreement)
- **Percentile Rank**: 16.4th percentile
- **Score Range**: 3.0 - 4.0 (spread: 1.0)

**Interpretation:**
- Scores in the **16th percentile** on Grit company-wide (top 84%)
- Peer average (3.67) is **-0.67 below team average**
- **Moderate agreement** among reviewers (SD: 0.47)
- Score range (3.0-4.0) shows reasonable consistency in reviewer perceptions

### Question 9: Growth Mindset

**Basic Scores:**
- **Peers Average**: 4.33 (based on 3 ratings)
- **Response Rate**: 3/4 peer reviewers (75%)
- **Self Score**: 4.0
- **Median Score**: 4.00

**Self-Awareness Deltas:**
- **Delta (Self - Peers)**: -0.33 ðŸŸ¢ (Humble)
- **Delta (Self - Level)**: -0.16 ðŸŸ¢ (Humble)
- **Delta (Self - Team)**: -0.42 ðŸŸ¢ (Humble)
- **Delta (Self - Department)**: -0.21 ðŸŸ¢ (Humble)
- **Delta (Self - Project)**: -0.20 ðŸŸ¢ (Humble)

**Performance Comparisons (Peer Avg vs Groups):**
- **vs Team Average**: -0.08 (Team avg: 4.42) - Below team
- **vs Project Average**: +0.13 (Project avg: 4.20) - Above project
- **vs Department Average**: +0.12 (Department avg: 4.21) - Above department
- **vs Company Average**: +0.14 (Company avg: 4.19) - Above company

**Statistical Analysis:**
- **Standard Deviation**: 0.47 (Moderate agreement)
- **Percentile Rank**: 56.2th percentile
- **Score Range**: 4.0 - 5.0 (spread: 1.0)

**Interpretation:**
- Scores in the **56th percentile** on Growth Mindset company-wide (top 44%)
- Peer average (4.33) is **+0.14 above company average**
- **Moderate agreement** among reviewers (SD: 0.47)
- Score range (4.0-5.0) shows reasonable consistency in reviewer perceptions

### Question 10: No Unnecessary Rules

**Basic Scores:**
- **Peers Average**: 4.33 (based on 3 ratings)
- **Response Rate**: 3/4 peer reviewers (75%)
- **Self Score**: 3.0
- **Median Score**: 4.00

**Self-Awareness Deltas:**
- **Delta (Self - Peers)**: -1.33 ðŸŸ¢ (Humble)
- **Delta (Self - Level)**: -1.03 ðŸŸ¢ (Humble)
- **Delta (Self - Team)**: -1.42 ðŸŸ¢ (Humble)
- **Delta (Self - Department)**: -1.06 ðŸŸ¢ (Humble)
- **Delta (Self - Project)**: -1.17 ðŸŸ¢ (Humble)

**Performance Comparisons (Peer Avg vs Groups):**
- **vs Team Average**: -0.08 (Team avg: 4.42) - Below team
- **vs Project Average**: +0.17 (Project avg: 4.17) - Above project
- **vs Department Average**: +0.27 (Department avg: 4.06) - Above department
- **vs Company Average**: +0.22 (Company avg: 4.11) - Above company

**Statistical Analysis:**
- **Standard Deviation**: 0.47 (Moderate agreement)
- **Percentile Rank**: 64.0th percentile
- **Score Range**: 4.0 - 5.0 (spread: 1.0)

**Interpretation:**
- Scores in the **64th percentile** on No Unnecessary Rules company-wide (top 36%)
- Peer average (4.33) is **+0.27 above department average**
- **Moderate agreement** among reviewers (SD: 0.47)
- Score range (4.0-5.0) shows reasonable consistency in reviewer perceptions

### Question 11: eNPS (Employee Net Promoter Score)

**Basic Scores:**
- **Peers Average**: 4.75 (based on 4 ratings)
- **Response Rate**: 4/4 peer reviewers (100%)
- **Self Score**: 5.0
- **Median Score**: 5.00

**Self-Awareness Deltas:**
- **Delta (Self - Peers)**: +0.25 ðŸ”´ (Overconfident)
- **Delta (Self - Level)**: +0.40 ðŸ”´ (Overconfident)
- **Delta (Self - Team)**: +0.12 ðŸ”´ (Overconfident)
- **Delta (Self - Department)**: +0.31 ðŸ”´ (Overconfident)
- **Delta (Self - Project)**: +0.12 ðŸ”´ (Overconfident)

**Performance Comparisons (Peer Avg vs Groups):**
- **vs Team Average**: -0.12 (Team avg: 4.88) - Below team
- **vs Project Average**: -0.13 (Project avg: 4.88) - Below project
- **vs Department Average**: +0.06 (Department avg: 4.69) - Above department
- **vs Company Average**: +0.08 (Company avg: 4.67) - Above company

**Statistical Analysis:**
- **Standard Deviation**: 0.43 (Moderate agreement)
- **Percentile Rank**: 50.0th percentile
- **Score Range**: 4.0 - 5.0 (spread: 1.0)

**Interpretation:**
- Scores in the **50th percentile** on eNPS (Employee Net Promoter Score) company-wide (top 50%)
- Peer average (4.75) is **-0.13 below project average**
- **Moderate agreement** among reviewers (SD: 0.43)
- Score range (4.0-5.0) shows reasonable consistency in reviewer perceptions

---

## Question 11: eNPS - Peer Comments Summary

### Synthesized Summary

*[AI synthesis needed - invoke rise8-assessment-reviewer agent with eNPS comments]*

<details>
<summary>View Raw eNPS Comments</summary>

**Alex Berner:**
Damon is relatively new to our team but he has been a strong contributor and a great personality thus far. He has one of the strongest growth mindsets I've worked alongside at Rise8 that drives him to constantly experiment and learn, especially with AI tools, and he has a vast array of design tools and practices to aid with user research and prototyping. I have been impressed with what I have seen so far

**Abbie Burton:**
Damon joined the team in August and has already made substantial impact. His thoughtfulness, preparation, and research-driven approach have elevated the quality of our work and strengthened our connection with users.




He collaborated on two site visits, one in person and one for which we prepared content for Alex and Frank. For the in-person visit, Damon created templates for observations, time tracking, SEQ, enabling us to gather baseline metrics on the current Vue app and capture valuable quantitative data. He also developed JTBD questions to ensure we asked the right questions to uncover meaningful insights. Additionally, he created a user interview cheat sheet to help developers who are less familiar with user research feel confident and prepared.




For the second site visit, Damon introduced a new research method, the highlighter test, to help us better understand how users interact with a specific section of the app and how they perceive the relationships between data. Because of his preparation, Frank and Alex felt confident and were able to conduct all the necessary research on our behalf.




Damon has also been innovative in leveraging AI to build a local prototype, enabling rapid iteration and testing ahead of our WC migration. His design decisions are consistently backed by data and research, and he always strives to elevate the user experience. He actively collaborates with other S31 designers, bringing back best practices and patterns from their applications to strengthen our own.




He has also taken point on user communications, initiating a communication strategy tracker so we can maintain clear, organized, and consistent engagement with our users.




Overall, Damonâ€™s creativity, rigor, and collaboration have already become invaluable to the team.

**Darius DeSpain:**
Damon is always kind and helpful. He is on our sister team so I haven't had the opportunity to observe everything, but he has shown me that he cares about the mission and will do what's required to create impact.

**Ian Sperry:**
I would rehire Damon because he's a highly proactive, innovative, and indispensable designer who consistently set the quality bar for our product. He possesses the core skills and the growth mindset necessary to make an impact.




Damonâ€™s work delivers excellent user experiences that are frequently cited as a major differentiator for the product. His high standards and bias toward action ensure we are always shipping high-quality solutions.




He actively acts as a leader in innovation by exploring and integrating new tools, like his initiative with AI in his design workflows. This shows he's not just executing tasks; he's thinking forward and trying to level up our collective process.




He's a dependable teammate, always willing to take on additional tasks and consistently seeks feedback to improve.


</details>

---

## Question 12: Start/Stop/Keep Recommendations

### Synthesized Summary

**START:**
*[AI synthesis needed - invoke rise8-assessment-reviewer agent with START feedback]*

**STOP:**
*[AI synthesis needed - invoke rise8-assessment-reviewer agent with STOP feedback]*

**KEEP:**
*[AI synthesis needed - invoke rise8-assessment-reviewer agent with KEEP feedback]*

<details>
<summary>View Raw Start/Stop/Keep Feedback</summary>

**Alex Berner:**
Start turning personal experiments and ideation into documented measurable outcomes to track for the team. You have a lot of drive to innovate and experiment, so there's a great opportunity to align that more closely with Rise8 delivery artifacts and have a scalable and repeatable framework for incepting, progressing, and submitting outcomes. I think you're an awesome person to figure this out for Section 31




Keep up the user research skills and methods you've brought to Section 31, and help share those out among other teams who are less UX inclined and struggling with how to make the most out of limited time with users. The artifacts and exercises you have put together can work super effectively even for engineers going onsite alone which is beneficial to our limited classified environment access, so could even be a playbook for others to use. My only word of caution here is to personally not stick too tightly to the frameworks, and be flexible and adaptable to get what you can from the users even if it's scrappy and outside the box

**Abbie Burton:**
- Start the OA process early, recognizing that it will take significant time, and leverage Blitzarâ€™s experience to help ensure a smoother path for Malibu.
- Continue using new research methodologies during site visits to uncover deeper insights into user needs.
- Continue creating quick prototypes to test ideas rapidly, focusing on functionality rather than perfection since the final versions will be built in WC.

**Darius DeSpain:**
- start
   - encouraging other product teams to do site visits. Malibu is the only team that does them consistently
- stop
   - Nothing I can think of here
- keep doing
   - designing amazing UI for Malibu. Your UI designs are so good that I'm advising we steal them for Starfox.
   - Pairing with Tom when helpful to work on reusable work between teams

**Ian Sperry:**
Keep delivering high-quality, differentiating user experiences. He should also continue leading the way in integrating new technologies like AI into his daily work, making sure he stays ahead of the curve in terms of tooling and methodology.




I can't think of anything Damon should stop doing but I will say he should be wary of reacting and pivoting priorities too quickly when recieving new information. Patience and focusing on strategic goals is the key.


</details>

---

## Report Metadata

- **Generated**: 2025-12-04 17:26:23
- **Script Version**: 1.0.0
- **Data Sources**:
  - Assessment File: `assessments/Design/Damon_Redding.md`
  - Team Mapping: `LatticeAPI/lattice_api_client/team_map.json`
  - Comprehensive Data: `docs/analysis-results/riser_data_detailed.csv`
  - Tier Assignments: `docs/analysis-results/tier_assignments.csv`
